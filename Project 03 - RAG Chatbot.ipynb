{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ei_N_jlUvWDV",
    "outputId": "721b8849-1cab-4a36-a655-754bb57517e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install langchain\n",
    "!pip install torch\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install huggingface-hub\n",
    "!pip install pypdf\n",
    "!pip -q install accelerate\n",
    "!pip install llama-cpp-python\n",
    "!pip -q install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8D6s3uMHv-NG"
   },
   "outputs": [],
   "source": [
    "# Import classes\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fEXsAyTZxBu7"
   },
   "outputs": [],
   "source": [
    "# Create a PyPDFDirectoryLoader object with the directory path \"Data/\"\n",
    "loader = PyPDFDirectoryLoader(\"Data/\")\n",
    "\n",
    "# Load data using the PyPDFDirectoryLoader object\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "T1L27WlvyJIu"
   },
   "outputs": [],
   "source": [
    "# Create a RecursiveCharacterTextSplitter object with max size of each chunk and overlap between chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "\n",
    "# Split documents into chunks using the RecursiveCharacterTextSplitter object\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6Jq5TiVcyiu2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of text chunks generated by the splitting process\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1o8aXL-Ryp9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='European Parliament\\n2019-2024\\nTEXTS ADOPTED\\nP9_TA(2024)0138\\nArtificial Intelligence Act\\nEuropean Parliament legislative resolution of 13 March 2024 on the proposal for a \\nregulation of the European Parliament and of the Council on laying down harmonised \\nrules on Artificial Intelligence (Artificial Intelligence Act) and amending certain Union \\nLegislative Acts (COM(2021)0206 – C9-0146/2021 – 2021/0106(COD))\\n(Ordinary legislative procedure: first reading)\\nThe European Parliament,\\n– having regard to the Commission proposal to Parliament and the Council \\n(COM(2021)0206),\\n– having regard to Article 294(2) and Articles 16 and 114 of the Treaty on the \\nFunctioning of the European Union, pursuant to which the Commission submitted the \\nproposal to Parliament (C9-0146/2021),\\n– having regard to Article 294(3) of the Treaty on the Functioning of the European Union,\\n– having regard to the opinion of the European Central Bank of 29 December 20211,\\n– having regard to the opinion of the European Economic and Social Committee of 22 \\nSeptember 20212,\\n– having regard to the provisional agreement approved by the committees responsible \\nunder Rule 74(4) of its Rules of Procedure and the undertaking given by the Council \\nrepresentative by letter of 2 February 2024 to approve Parliament’s position, in \\naccordance with Article 294(4) of the Treaty on the Functioning of the European Union,\\n– having regard to Rule 59 of its Rules of Procedure,\\n– having regard to the joint deliberations of the Committee on Internal Market and \\nConsumer Protection and the Committee on Civil Liberties, Justice and Home Affairs \\nunder Rule 58 of the Rules of Procedure,\\n– having regard to the opinion of the Committee on Industry, Research and Energy, the \\nCommittee on Culture and Education, the Committee on Legal Affairs, the Committee \\n1 OJ C 115, 11.3.2022, p. 5.\\n2 OJ C 517, 22.12.2021, p. 56.', metadata={'source': 'Data\\\\The-AI-Act-Final.pdf', 'page': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the first text chunk generated by the splitting process\n",
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M7oKKdTPyxax"
   },
   "outputs": [],
   "source": [
    "# Initialize HuggingFaceEmbeddings with the specified model_name\n",
    "# This will load the pre-trained model \"all-MiniLM-L6-v2\" for generating embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MJbn_hjzznhm"
   },
   "outputs": [],
   "source": [
    "# Create a FAISS vector store from the text chunks using the specified embeddings\n",
    "# This will generate vectors for each text chunk based on the provided embeddings\n",
    "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v9Y5Va538RDL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "#   Initialize LlamaCpp with specified parameters:\n",
    "#   - streaming: Enables streaming mode for continuous input processing.\n",
    "#   - model_path: Path to the LLM model file \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\".\n",
    "#   - temperature: Softmax temperature for controlling generation randomness.\n",
    "#   - top_p: Top-p (nucleus) sampling threshold for controlling generation diversity.\n",
    "#   - verbose: Enables verbose mode for detailed logging.\n",
    "#   - n_ctx: Size of the input context window for the model.\n",
    "llm = LlamaCpp(\n",
    "    streaming = True,\n",
    "    model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    temperature=0.85,\n",
    "    top_p=1,\n",
    "    verbose=True,\n",
    "    n_ctx=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4Vpb_w-i94Th"
   },
   "outputs": [],
   "source": [
    "# Create a RetrievalQA object using the specified parameters:\n",
    "#   - llm: LlamaCpp instance used for language model inference.\n",
    "#   - chain_type: Type of QA chain to create (\"stuff\" in this case).\n",
    "#   - retriever: Vector store converted to a retriever with search arguments {\"k\": 2}.\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query for the QA system to process\n",
    "query_01 = \"What is the purpose of the AI Act?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15356.83 ms\n",
      "llama_print_timings:      sample time =      27.89 ms /    90 runs   (    0.31 ms per token,  3226.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =  354614.48 ms /   515 tokens (  688.57 ms per token,     1.45 tokens per second)\n",
      "llama_print_timings:        eval time =   17266.07 ms /    89 runs   (  194.00 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =  372631.41 ms /   604 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The purpose of the AI Act is to improve the functioning of the internal market and promote the uptake of human-centric and trustworthy artificial intelligence (AI), while ensuring a high level of protection of health, safety, fundamental rights enshrined in the Charter of Fundamental Rights, including democracy, the rule of law and environmental protection, against the harmful effects of AI systems in the Union, and to support innovation.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the QA system on the provided query to retrieve an answer\n",
    "qa.run(query_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8DWiBe3cENZB"
   },
   "outputs": [],
   "source": [
    "# Define a query for the QA system to process\n",
    "query_02 = \"I want to share AI-generated content, does it have to be tagged somehow? Can you give me the original article in the AI Act that you have used as a source?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mdFYMQfR_gzI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   15356.83 ms\n",
      "llama_print_timings:      sample time =      85.33 ms /   256 runs   (    0.33 ms per token,  3000.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =  281770.90 ms /   648 tokens (  434.83 ms per token,     2.30 tokens per second)\n",
      "llama_print_timings:        eval time =   51928.48 ms /   256 runs   (  202.85 ms per token,     4.93 tokens per second)\n",
      "llama_print_timings:       total time =  335597.25 ms /   904 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" No, it doesn't have to be tagged. The question is asking for a summary of the content used for training a general-purpose AI model and that information does not need to be tagged. Here is the relevant paragraph from Article 105 of the AI Act: (105) General-purpose models, in particular large generative models, capable of generating text, images, and other content, present unique innovation opportunities but also challenges to artists, authors, and other creators and the way their creative content is created, distributed, used and consumed. The development and training of such models require access to vast amounts of text, images, videos, and other data. Text and data mining techniques may be used extensively in this context for the retrieval and analysis of such content, which may be protected by copyright and related rights. Any use of copyright protected content requires the authorisation of the rightsholder concerned unless relevant copyright exceptions and limitations apply. Directive (EU) 2019/790 introduced exceptions and limitations allowing reproductions and extractions of works or other subject matter, for the purpose of text and data mining, under certain conditions. Under these rules, rightsholders may choose to reserve\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the QA system on the provided query to retrieve an answer\n",
    "qa.run(query_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query for the QA system to process\n",
    "query_03 = \"Can you give me some examples of unacceptable AI practices according to the AI Act?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   15356.83 ms\n",
      "llama_print_timings:      sample time =      28.12 ms /    93 runs   (    0.30 ms per token,  3306.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =  115448.80 ms /   523 tokens (  220.74 ms per token,     4.53 tokens per second)\n",
      "llama_print_timings:        eval time =   18121.56 ms /    92 runs   (  196.97 ms per token,     5.08 tokens per second)\n",
      "llama_print_timings:       total time =  134362.13 ms /   615 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" According to the AI Act, one example of an unacceptable AI practice is placing on the market, putting into service or using an AI system that deploys subliminal techniques beyond a person's consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of, materially distorting the behavior of a person or a group of persons by appreciably impairing their ability to make an informed decision.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute the QA system on the provided query to retrieve an answer\n",
    "qa.run(query_03)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
